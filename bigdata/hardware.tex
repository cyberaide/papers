\FILE{hardware.tex}

\subsection{Hardware Overview}

According to the manual, FutureGrid is build out of a number of clusters of different type and size that are interconected with up to a 10GB Ethernet among its sites. The sites include Indiana University, University of Chicago, San Diego Supercomputing Center, Texas Advanced Computing Center, and University of Florida.

\subsubsection{Overview of the Clusters}\label{S:hw-cluster} 

\begin{table}[htb]

\caption{FutureGrid Compute Resources}\label{T:hw}

\begin{center}
\begin{tabular}{rrrrrrrrr}
Name    & System Type                &  \rot{Nodes} &  \rot{CPUS}   & \rot{Cores}   & \rot{TFLOPS}  & \rot{RAM (GB)}        & \rot{Storage (TB)}    & Site \\
\hline
india   & IBM iDataplex              & 128          & 256     & 1024    & 11      & 3072            & 335             & IU \\
hotel   & IBM iDataplex              & 84           & 168     & 672     & 7       & 2016            & 120             & UC \\
sierra  & IBM iDataplex              & 84           & 168     & 672     & 7       & 2688            & 96              & SDSC \\
foxtrot & IBM iDataplex              & 32           & 64      & 256     & 3       & 768             & 0               & UF \\
alamo   & Dell Poweredge             & 96           & 192     & 768     & 8       & 1152            & 30              & TACC \\
xray    & Cray XT5m                  & 1            & 166     & 664     & 6       & 1328            & 5.4             & IU \\
bravo   & HP Proliant                & 16           & 32      & 128     & 1.7     & 3072            & 128             & IU \\
delta   & \shortstack{SuperMicro\\ GPU Cluster}     & 16           & 32      & 192     &         & 1333            & 144             & IU \\
lima    & Aeon Eclipse64             & 8            & 16      & 128     & 1.3     & 512             & 3.8             & SDSC \\
echo    & \shortstack{SuperMicro \\ScaleMP Cluster} & 16           & 32      & 192     & 2       & 6144            & 192             & IU \\
\end{tabular}
\end{center}
\end{table}


\input{hw-table}



\subsubsection{Overview of Networking}

The significant number of distinct systems within FutureGrid provide a
heterogeneous distributed architecture and are connected by
high-bandwidth network links supporting distributed system research
\cite{las12fg-bookchapter}. FutureGrids network used to have a
dedicated network between sites \cite{las12fg-bookchapter}. However,
the network infrastructure has recently changed due to changes as part
of its major network operator the National Lambda Rail.  Due to these
changes the operation of the network between the sites conducted by
the national lambda rail has switched opertaion to XSEDE and are no
longer exclusive. This is no major handicap for the projects conducted
on FutureGrid based on our project portfolio.  The current high level
network diagram is depicted in Figure~\ref{F:network}.

The core resources to FutureGrid at SDSC, IU, TACC, and UF are now all
connected via the XSEDE network and integrated via the FG core router
in Chicago. WIthin The IU network additional clusters are integrated
and are described in more detail in Section~\ref{S:hw-cluster}. 

A Spirent H10 XGEM Network Impairment emulator
\cite{www-network-impairment} can be colocated with resources at
Indiana University, to enable experiments to include network latency,
jitter, loss, and errors to network traffic.

In addition we have added several shaded components that are related
to a spcial software service called cloudmesh that we explain in more
detail in Section~\ref{S:cloudmesh}.


\begin{figure}[htb]
  \centering
    \includegraphics[width=1.0\textwidth]{images/fg-network-2014-cm.pdf}
  \caption{High level network diagram and conceptual integration of cloudmesh resources.}
\label{F:network}
\end{figure}

\subsubsection{Overview of Storage}

FutureGrid has only a very limited amount of storage space and users
are requested to remove their storage space after use. GutureGrid does
not provide capacity for long term storage or long term
experiments. Users with special needs may be acomodated by special
storage setups. The list of storage services is shown in Table \ref{T:storage}.

\begin{table}[htb]
\caption{Storage Resources of FutureGrid.}
\label{T:storage} 

\centering{}%
\begin{tabular}{lrll}
\textbf{System Type } & \textbf{Capacity(TB) } & \textbf{File System } & \textbf{Site }\tabularnewline
\hline 
Xanadu 360  & 180  & NFS  & IU \tabularnewline
DDN 6620  & 120  & GPFS  & UC \tabularnewline
Sunfire x4170  & 96  & ZFS  & SDSC \tabularnewline
Dell MD3000  & 30  & NFS  & TACC \tabularnewline
IBM dx360 M3  & 24  & NFS  & UF \tabularnewline
\end{tabular}
\end{table}

